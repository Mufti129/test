# dashboard_final_refactor.py
# =============================
# STEP 1: ENVIRONMENT DETECTION
# =============================
import os
ENV = os.getenv("APP_ENV", "cloud").lower()
IS_CLOUD = ENV == "cloud"

import warnings
warnings.filterwarnings("ignore")

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns
from datetime import timedelta

# time-series classical
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing, Holt
from statsmodels.tsa.seasonal import seasonal_decompose

# sklearn
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error

# optional libs (wrapped)
try:
    from xgboost import XGBRegressor
    _HAS_XGB = True
except Exception:
    _HAS_XGB = False

try:
    from pmdarima import auto_arima
    _HAS_PMD = True
except Exception:
    _HAS_PMD = False

try:
    from prophet import Prophet
    _HAS_PROPHET = True
except Exception:
    _HAS_PROPHET = False

try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout
    _HAS_TF = True
except Exception:
    _HAS_TF = False

st.set_page_config(page_title="Dashboard Analisis Data (Refactor)", layout="wide")
st.title("ðŸ“Š Dashboard Produk â€” Forecasting (Refactor & Auto-Model)")

# -----------------------------
# Helpers
# -----------------------------
@st.cache_data
def load_data(gsheet_url):
    return pd.read_csv(gsheet_url)

def beautify_timeseries_plot(ax, title="", ylabel="Value"):
    ax.set_title(title, fontsize=12)
    ax.set_ylabel(ylabel)
    ax.grid(True, alpha=0.3)
    locator = mdates.AutoDateLocator()
    formatter = mdates.ConciseDateFormatter(locator)
    ax.xaxis.set_major_locator(locator)
    ax.xaxis.set_major_formatter(formatter)
    plt.setp(ax.get_xticklabels(), rotation=45)

def evaluate_series(true, pred):
    true = np.array(true)
    pred = np.array(pred)
    rmse = np.sqrt(np.mean((pred - true)**2))
    mae = np.mean(np.abs(pred - true))
    with np.errstate(divide='ignore', invalid='ignore'):
        mape = np.mean(np.abs((pred - true) / np.where(true==0, np.nan, true))) * 100
    return rmse, mae, mape

def remove_outliers_iqr(series, multiplier=1.5):
    q1 = np.nanpercentile(series, 25)
    q3 = np.nanpercentile(series, 75)
    iqr = q3 - q1
    low = q1 - multiplier * iqr
    high = q3 + multiplier * iqr
    series_clipped = np.where(series < low, low, np.where(series > high, high, series))
    return series_clipped

def create_lag_features(df, col='value', lags=[1,7,14,30]):
    df_feat = df.copy()
    for l in lags:
        df_feat[f'lag_{l}'] = df_feat[col].shift(l)
    df_feat['dayofweek'] = df_feat.index.dayofweek
    df_feat['day'] = df_feat.index.day
    df_feat['month'] = df_feat.index.month
    df_feat = df_feat.dropna()
    return df_feat

# -----------------------------
# Load Google Sheet
# -----------------------------
GSHEET_URL = st.text_input("Masukkan CSV export Google Sheet URL (atau biarkan default):",
                           value="https://docs.google.com/spreadsheets/d/1APilL0UzyGIBslMDIPF7B2Ftvo7XK0lo6q_d2IOjW1A/export?format=csv&gid=1146076705")

try:
    df = load_data(GSHEET_URL)
except Exception as e:
    st.error(f"Gagal load Google Sheet: {e}")
    st.stop()

required_cols = ["Tgl. Pesanan", "Nama Barang", "QTY", "Nominal"]
missing = [c for c in required_cols if c not in df.columns]
if missing:
    st.error(f"Kolom berikut tidak ditemukan di sheet: {missing}")
    st.stop()

df["Tgl. Pesanan"] = pd.to_datetime(df["Tgl. Pesanan"], errors="coerce")
df = df.dropna(subset=["Tgl. Pesanan"])

# -----------------------------
# Sidebar UX
# -----------------------------
st.sidebar.header("âš™ï¸ Pengaturan Dashboard")
st.sidebar.markdown("### Environment")
st.sidebar.write("Mode:", "â˜ï¸ Cloud (Safe)" if IS_CLOUD else "ðŸ’» Local (Full)")

# Date range
min_date, max_date = df["Tgl. Pesanan"].min(), df["Tgl. Pesanan"].max()
date_range = st.sidebar.date_input("Filter Rentang Tanggal", value=(min_date, max_date))
if len(date_range) != 2:
    st.error("Pilih rentang tanggal dengan benar.")
    st.stop()
start_date, end_date = date_range

# Product selection
products = sorted(df["Nama Barang"].dropna().unique().tolist())
prod_multi = st.sidebar.multiselect("Pilih Nama Barang (boleh lebih dari 1):", options=products, default=None)

# Metric selection
metric_choice = st.sidebar.radio("Pilih metrik:", ["QTY", "Nominal"])

# Preprocessing options
st.sidebar.markdown("### Preprocessing / Cleansing")
apply_outlier = st.sidebar.checkbox("Remove outliers (IQR clipping)", value=True)
apply_log = st.sidebar.checkbox("Apply log1p transform (improves stability)", value=False)
apply_smoothing = st.sidebar.checkbox("Apply smoothing (rolling mean)", value=False)
smoothing_window = st.sidebar.slider("Smoothing window (days)", 3, 30, 7)

# Modeling options
st.sidebar.markdown("### Modeling Options")
enable_auto_model = st.sidebar.checkbox("Enable Auto Model Selection (compare RMSE)", value=True)
include_heavy_models = st.sidebar.checkbox("Include heavy models (XGBoost / LSTM) if available", value=False)
lstm_toggle = st.sidebar.checkbox("Enable LSTM (only if TensorFlow is installed)", value=False)

# Analysis selector
st.sidebar.markdown("### Analisis")
analysis = st.sidebar.radio("Pilih Analisis:", ["Preview Data", "Descriptive", "Correlation", "Forecasting"])

# -----------------------------
# Apply filters
# -----------------------------
df_filtered = df[(df["Tgl. Pesanan"] >= pd.to_datetime(start_date)) & (df["Tgl. Pesanan"] <= pd.to_datetime(end_date))]
if prod_multi:
    df_filtered = df_filtered[df_filtered["Nama Barang"].isin(prod_multi)]

st.subheader("ðŸ“„ Preview Data Setelah Filter")
st.dataframe(df_filtered.head(300))

# -----------------------------
# Numeric & quick checks
# -----------------------------
if df_filtered.empty:
    st.error("Data kosong setelah filter. Cek rentang tanggal / produk.")
    st.stop()

# -----------------------------
# Descriptive & Correlation
# -----------------------------
if analysis == "Descriptive":
    st.header("ðŸ“˜ Descriptive Analysis")
    st.write(df_filtered.describe(include='all'))
    num_cols = df_filtered.select_dtypes(include=[np.number]).columns.tolist()
    if num_cols:
        chosen = st.multiselect("Pilih kolom numerik untuk plot histogram:", num_cols, default=num_cols[:3])
        for c in chosen:
            fig, ax = plt.subplots()
            ax.hist(df_filtered[c].dropna(), bins=30)
            ax.set_title(f"Distribusi {c}")
            st.pyplot(fig)

elif analysis == "Correlation":
    st.header("ðŸ“— Correlation Analysis")
    num_cols = df_filtered.select_dtypes(include=[np.number]).columns.tolist()
    if len(num_cols) < 2:
        st.warning("Perlu minimal 2 kolom numerik.")
    else:
        chosen = st.multiselect("Pilih kolom untuk korelasi:", num_cols, default=num_cols[:5])
        if len(chosen) >= 2:
            corr = df_filtered[chosen].corr()
            st.dataframe(corr)
            fig, ax = plt.subplots(figsize=(10,6))
            sns.heatmap(corr, annot=True, fmt=".2f", cmap="Blues", ax=ax)
            st.pyplot(fig)

# -----------------------------
# Forecasting
# -----------------------------
else:
    st.header("ðŸ“ˆ Forecasting per Produk / Grup")

    metric = metric_choice
    st.write(f"Metric for forecasting: **{metric}**")

    # Aggregate per day
    df_daily = df_filtered[["Tgl. Pesanan", metric]].groupby("Tgl. Pesanan").sum()
    ts_daily = df_daily.resample("D").sum()

    # Replace zeros (sparse) optionally or keep zeros but ffill option
    zero_count = (ts_daily[metric] == 0).sum()
    st.write(f"Total days: {len(ts_daily)} â€” days with 0 {metric}: {zero_count}")

    # Optionally replace 0 with NaN then ffill/bfill to reduce sparsity
    if st.sidebar.checkbox("Treat zeros as missing (ffill)", value=True):
        ts_daily[metric] = ts_daily[metric].replace(0, np.nan)
        ts_daily[metric] = ts_daily[metric].ffill().bfill().fillna(0)

    # Outlier removal
    if apply_outlier:
        ts_daily['value_raw'] = ts_daily[metric].values
        ts_daily['value'] = remove_outliers_iqr(ts_daily['value_raw'])
    else:
        ts_daily['value'] = ts_daily[metric].values

    # Log transform
    if apply_log:
        ts_daily['value'] = np.log1p(ts_daily['value'])

    # Smoothing
    if apply_smoothing:
        ts_daily['value'] = ts_daily['value'].rolling(window=smoothing_window, min_periods=1, center=False).mean()

    # Decomposition (if enough points)
    if len(ts_daily) >= 14:
        try:
            decomp = seasonal_decompose(ts_daily['value'], period=7, model='additive', extrapolate_trend='freq')
            st.subheader("Decomposition (additive, period=7)")
            fig_d, axes = plt.subplots(4,1, figsize=(12,8), sharex=True)
            axes[0].plot(decomp.observed); axes[0].set_title("Observed")
            axes[1].plot(decomp.trend); axes[1].set_title("Trend")
            axes[2].plot(decomp.seasonal); axes[2].set_title("Seasonal")
            axes[3].plot(decomp.resid); axes[3].set_title("Residual")
            beautify_timeseries_plot(axes[-1], title="Decomposition (dates)", ylabel=metric)
            st.pyplot(fig_d)
        except Exception as e:
            st.info(f"Decomposition error: {e}")

    # Basic stats & histogram
    st.subheader("Summary Statistik Harian (setelah cleaning)")
    st.write(ts_daily['value'].describe())
    fig_hist, ax_hist = plt.subplots()
    ax_hist.hist(ts_daily['value'].dropna(), bins=40)
    ax_hist.set_title("Histogram nilai harian")
    st.pyplot(fig_hist)

    # Plot series
    st.subheader("Time Series Harian")
    fig_ts, ax_ts = plt.subplots(figsize=(14,3))
    ax_ts.plot(ts_daily.index, ts_daily['value'], label="Actual")
    beautify_timeseries_plot(ax_ts, title=f"Daily Series - {metric}", ylabel=metric)
    ax_ts.legend()
    st.pyplot(fig_ts)

    # Check for all zeros
    if np.allclose(ts_daily['value'].fillna(0).values, 0):
        st.warning("Nilai target semuanya nol setelah preprocessing â€” forecasting tidak akurat.")
        st.stop()

    # Train-test split
    train_size = int(len(ts_daily) * 0.8)
    if train_size < 10:
        st.warning("Data terlalu sedikit untuk model yang kompleks. Pertimbangkan agregasi mingguan.")
    train = ts_daily.iloc[:train_size].copy()
    test = ts_daily.iloc[train_size:].copy()

    # Models list (dynamically adjusted)
    base_models = ["ARIMA", "SES", "Holt", "Holt-Winters", "MovingAverage", "Naive", "LinearRegression", "RandomForest"]
    ml_models = []
    if _HAS_XGB and include_heavy_models:
        ml_models.append("XGBoost")
    if include_heavy_models and _HAS_TF:
        ml_models.append("LSTM")
    all_models = base_models + ml_models

    # Allow user to pick models or auto-run
    st.subheader("Model Selection")
    chosen_models = st.multiselect("Pilih model (kosong = jalankan auto-selection)", options=all_models, default=None)
    if not chosen_models:
        chosen_models = all_models if enable_auto_model else base_models

    st.write("Models to run:", chosen_models)

    # Function that runs a single model and returns test_forecast and future_forecast series
    def run_model(name, train_ser, test_ser, period=30):
        """Return (test_forecast_series, future_forecast_series, info_dict)"""
        info = {"model": name}
        try:
            if name == "ARIMA":
                model = ARIMA(train_ser, order=(2,1,2))
                fit = model.fit()
                test_fc = fit.forecast(steps=len(test_ser))
                future_fc = fit.forecast(steps=period)
                test_fc = pd.Series(test_fc, index=test_ser.index)
                future_fc = pd.Series(future_fc, index=pd.date_range(test_ser.index[-1]+timedelta(days=1), periods=period, freq='D'))
                info['note'] = "ARIMA(2,1,2)"

            elif name == "SES":
                model = SimpleExpSmoothing(train_ser)
                fit = model.fit()
                test_fc = fit.forecast(steps=len(test_ser))
                future_fc = fit.forecast(steps=period)
                test_fc = pd.Series(test_fc, index=test_ser.index)
                future_fc = pd.Series(future_fc, index=pd.date_range(test_ser.index[-1]+timedelta(days=1), periods=period, freq='D'))

            elif name == "Holt":
                model = Holt(train_ser)
                fit = model.fit()
                test_fc = fit.forecast(steps=len(test_ser))
                future_fc = fit.forecast(steps=period)
                test_fc = pd.Series(test_fc, index=test_ser.index)
                future_fc = pd.Series(future_fc, index=pd.date_range(test_ser.index[-1]+timedelta(days=1), periods=period, freq='D'))

            elif name == "Holt-Winters":
                sp = 7 if len(train_ser) >= 14 else None
                if sp:
                    model = ExponentialSmoothing(train_ser, trend="add", seasonal="add", seasonal_periods=sp)
                else:
                    model = ExponentialSmoothing(train_ser, trend="add", seasonal=None)
                fit = model.fit()
                test_fc = fit.forecast(steps=len(test_ser))
                future_fc = fit.forecast(steps=period)
                test_fc = pd.Series(test_fc, index=test_ser.index)
                future_fc = pd.Series(future_fc, index=pd.date_range(test_ser.index[-1]+timedelta(days=1), periods=period, freq='D'))

            elif name == "MovingAverage":
                window = 7
                last_ma = train_ser.rolling(window).mean().iloc[-1]
                test_fc = pd.Series([last_ma]*len(test_ser), index=test_ser.index)
                future_fc = pd.Series([last_ma]*period, index=pd.date_range(test_ser.index[-1]+timedelta(days=1), periods=period, freq='D'))

            elif name == "Naive":
                last = train_ser.iloc[-1]
                test_fc = pd.Series([last]*len(test_ser), index=test_ser.index)
                future_fc = pd.Series([last]*period, index=pd.date_range(test_ser.index[-1]+timedelta(days=1), periods=period, freq='D'))

            elif name == "LinearRegression":
                X_train = np.arange(len(train_ser)).reshape(-1,1)
                y_train = train_ser.values
                X_test = np.arange(len(train_ser), len(train_ser)+len(test_ser)).reshape(-1,1)
                lr = LinearRegression(); lr.fit(X_train, y_train)
                test_pred = lr.predict(X_test)
                test_fc = pd.Series(test_pred, index=test_ser.index)
                X_future = np.arange(len(train_ser)+len(test_ser), len(train_ser)+len(test_ser)+period).reshape(-1,1)
                future_fc = pd.Series(lr.predict(X_future), index=pd.date_range(test_ser.index[-1]+timedelta(days=1), periods=period, freq='D'))
                info['estimator'] = lr

            elif name == "RandomForest":
                Xy = create_lag_features(pd.DataFrame(train_ser), col='value', lags=[1,7,14])
                X_train = Xy.drop(columns=['value']).values
                y_train = Xy['value'].values
                # build test features by concatenating end of train + test
                merged = pd.concat([train_ser, test_ser])
                merged_feat = create_lag_features(pd.DataFrame(merged), col='value', lags=[1,7,14])
                X_test = merged_feat.drop(columns=['value']).iloc[len(Xy):].values
                rf = RandomForestRegressor(n_estimators=200, random_state=42)
                rf.fit(X_train, y_train)
                test_pred = rf.predict(X_test)
                test_fc = pd.Series(test_pred, index=test_ser.index)
                # future using iterative prediction
                future_preds = []
                last_window = merged.values.flatten().tolist()
                for _ in range(period):
                    lag1 = last_window[-1]
                    lag7 = last_window[-7] if len(last_window)>=7 else last_window[0]
                    lag14 = last_window[-14] if len(last_window)>=14 else last_window[0]
                    feat = np.array([lag1, lag7, lag14, pd.Timestamp.max.dayofweek, 0, 0]).reshape(1,-1)  # simple placeholder for dows
                    # we will instead rely on model.predict with just lags shape
                    # build feat consistent with training (lag1,lag7,lag14,dayofweek,day,month) -- we approximate day features as zeros
                    try:
                        pred = rf.predict(feat)
                    except Exception:
                        pred = [last_window[-1]]
                    future_preds.append(pred[0])
                    last_window.append(pred[0])
                future_fc = pd.Series(future_preds, index=pd.date_range(test_ser.index[-1]+timedelta(days=1), periods=period, freq='D'))
                info['estimator'] = rf

            elif name == "XGBoost" and _HAS_XGB:
                X_train = np.arange(len(train_ser)).reshape(-1,1)
                y_train = train_ser.values
                X_test = np.arange(len(train_ser), len(train_ser)+len(test_ser)).reshape(-1,1)
                model = XGBRegressor(n_estimators=200, learning_rate=0.05)
                model.fit(X_train, y_train)
                pred = model.predict(X_test)
                test_fc = pd.Series(pred, index=test_ser.index)
                X_future = np.arange(len(train_ser)+len(test_ser), len(train_ser)+len(test_ser)+period).reshape(-1,1)
                future_fc = pd.Series(model.predict(X_future), index=pd.date_range(test_ser.index[-1]+timedelta(days=1), periods=period, freq='D'))
                info['estimator'] = model

            elif name == "LSTM" and _HAS_TF:
                # build sequences (simple)
                series = np.array(train_ser.fillna(method='ffill').values).flatten()
                window = 14
                Xs, ys = [], []
                for i in range(window, len(series)):
                    Xs.append(series[i-window:i])
                    ys.append(series[i])
                Xs, ys = np.array(Xs), np.array(ys)
                if len(Xs) < 10:
                    raise ValueError("Data too short for LSTM")
                Xs = Xs.reshape((Xs.shape[0], Xs.shape[1], 1))
                split = int(len(Xs)*0.8)
                X_train_l, y_train_l = Xs, ys
                # build model
                model = Sequential([
                    LSTM(64, return_sequences=True, input_shape=(window,1)),
                    Dropout(0.2),
                    LSTM(32, return_sequences=False),
                    Dropout(0.2),
                    Dense(16, activation='relu'),
                    Dense(1)
                ])
                model.compile(optimizer='adam', loss='mse')
                model.fit(X_train_l, y_train_l, epochs=30, batch_size=16, verbose=0)
                # prepare test sequences from the end of train + test
                combined = np.concatenate([train_ser.values.flatten(), test_ser.values.flatten()])
                X_test_seq = []
                for i in range(window, window+len(test_ser)):
                    seq = combined[i-window:i]
                    X_test_seq.append(seq)
                X_test_seq = np.array(X_test_seq).reshape((len(X_test_seq), window,1))
                pred_scaled = model.predict(X_test_seq).flatten()
                test_fc = pd.Series(pred_scaled, index=test_ser.index)
                # future iterative
                last_window = combined[-window:].tolist()
                future_preds = []
                for _ in range(period):
                    arr = np.array(last_window[-window:]).reshape((1,window,1))
                    p = model.predict(arr)[0][0]
                    future_preds.append(p)
                    last_window.append(p)
                future_fc = pd.Series(future_preds, index=pd.date_range(test_ser.index[-1]+timedelta(days=1), periods=period, freq='D'))
                info['estimator'] = model

            else:
                raise ValueError(f"Model {name} not available or not implemented")

            return test_fc, future_fc, info

        except Exception as e:
            return None, None, {"model": name, "error": str(e)}

    # Run models
    period = st.slider("Jumlah hari forecast masa depan:", 7, 180, 30)
    results = []
    progress = st.progress(0)
    for i, m in enumerate(chosen_models):
        progress.progress(int((i+1)/len(chosen_models)*100))
        test_fc, future_fc, info = run_model(m, train['value'], test['value'], period=period)
        if test_fc is None:
            st.warning(f"Model {m} gagal: {info.get('error')}")
            continue
        rmse, mae, mape = evaluate_series(test['value'].values, test_fc.values)
        results.append({"model": m, "rmse": rmse, "mae": mae, "mape": mape, "test_fc": test_fc, "future_fc": future_fc, "info": info})

    progress.empty()

    if not results:
        st.error("Tidak ada model yang berhasil dijalankan.")
        st.stop()

    # Show comparison table
    df_results = pd.DataFrame([{"Model":r["model"], "RMSE":r["rmse"], "MAE":r["mae"], "MAPE":r["mape"]} for r in results]).sort_values("RMSE")
    st.subheader("Perbandingan Model (diurutkan RMSE kecil -> besar)")
    st.dataframe(df_results.style.format({"RMSE":"{:.2f}", "MAE":"{:.2f}", "MAPE":"{:.2f}%"}))

    # Auto-select best
    best = min(results, key=lambda x: x['rmse'])
    st.success(f"Best model (by RMSE): {best['model']} â€” RMSE: {best['rmse']:.2f}, MAPE: {best['mape']:.2f}%")

    # Plot best model results
    st.subheader(f"Plot hasil model terbaik: {best['model']}")
    fig_b, ax_b = plt.subplots(figsize=(12,4))
    ax_b.plot(train.index, train['value'], label="Train")
    ax_b.plot(test.index, test['value'], label="Test Actual")
    ax_b.plot(best['test_fc'].index, best['test_fc'].values, label=f"Test Forecast ({best['model']})")
    beautify_timeseries_plot(ax_b, title=f"Train/Test vs Forecast - {best['model']}", ylabel=metric)
    ax_b.legend()
    st.pyplot(fig_b)

    # Show future forecast of best
    st.subheader(f"Future Forecast ({best['model']}) next {period} days")
    future_fc = best['future_fc']
    st.dataframe(future_fc.to_frame(name="Forecast"))

    fig_f, ax_f = plt.subplots(figsize=(14,4))
    # plot last N days of actual for context
    n_last = st.slider("Context window (days) for plotting actual:", 30, min(365, len(ts_daily)), 120)
    ts_zoom = ts_daily.tail(n_last)
    ax_f.plot(ts_zoom.index, ts_zoom['value'], label="Actual")
    ax_f.plot(future_fc.index, future_fc.values, label="Future Forecast")
    beautify_timeseries_plot(ax_f, title=f"Actual + Future Forecast ({best['model']})", ylabel=metric)
    ax_f.legend()
    st.pyplot(fig_f)

    # If log transform applied, remind user results are in log-scale
    if apply_log:
        st.warning("Transform log1p diterapkan pada data â€” hasil forecast dalam skala log1p. Untuk interpretasi, gunakan inverse np.expm1.")

    st.info("Selesai â€” Jika ingin memperbaiki akurasi, coba: tambah fitur lag yang lebih banyak, gunakan agregasi mingguan, atau tambahkan holiday features / external regressors.")

